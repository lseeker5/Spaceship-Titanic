{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nimport datetime\nimport math\nfrom sklearn.preprocessing import LabelEncoder\nimport statistics\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nfrom itertools import product\nfrom math import sqrt\nimport json\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#Models\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, model_from_json\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout,Reshape\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import clone_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n#Graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.signal import periodogram\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_pacf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample=pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')\ntest=pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\ntrain=pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=train.drop(columns=['Transported'])\ny=train['Transported']\n# X is Feature, Y is Target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nulls=[]\nunique=[]\n\nfor col in X.columns:\n    nulls.append(X[col].isnull().sum())\n    unique.append(X[col].nunique())\n    \nsummary=pd.DataFrame(data={'Columns':X.columns,'No of null':nulls,'No of unique values':unique})\nsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.pairplot(X.select_dtypes(include=['float64']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First lets split into deck num side ","metadata":{}},{"cell_type":"code","source":"# I'll check on how to do string formatting\ntrain_deck = [] \ntrain_count = []\ntest_deck = [] \ntest_count = []\n\ndeck = ['Cabin','Value Counts']\n\n# count_decks()\n# pd.set_option('display.max_rows', None)\n\n# print(, test['Cabin'].value_counts(), sep ='/t')\n\n# For train\n\ntrain_deck.append(train['Cabin'])\ntrain_count.append(train['Cabin'].value_counts())\n    \nsummary_train = pd.DataFrame(data = {deck[0] : train_deck , deck[1]: train_count})\nprint(summary_train)\n\n# For test\n\ntest_deck.append(test['Cabin'])\ntest_count.append(test['Cabin'].value_counts())\n\nsummary_test = pd.DataFrame(data = {deck[0]:test_deck, deck[1]: test_count})\nsummary_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saw above code - \n\n\n\nTrain & Test:\n\nGenerally, Age, FoodCourt, SPA, VRDeck shows best positive correlation with one another\n\nFor negative correlation, \n\n\n\n\n\n\nLasso Regression, L1/L2 Regression","metadata":{}},{"cell_type":"code","source":"test.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = test['Age'].isna()\nfeature = test.columns\n\ndef blank_data(file, locate):\n    return test.loc[missing, :]\nblank_data(test, missing)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = test['Age'].isna()\n\n# def blank_data(file, locate):\n#     return test.loc[missing]  \n# blank_data(test, missing)\n\ntest.dropna(how = 'any')\n    \ntest\n\n# TBC","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Arden's Edit:*\n\nStep 1.1: Define X & Y\n\nStep 1.2: Split Cabin into 3 separate columns\n\nStep 1.3: Splitting dataset into train and validation\n\nStep 2.1: Seeing which columns in X_train are in need of imputation/deletion\n\nFind out how to impute a catagorical column based on other columns","metadata":{}},{"cell_type":"code","source":"X=train.drop(columns=['Transported'])\nY=train['Transported']\n# X is Feature, Y is Target\n\nX_test = test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[\"Cabin\"]\nX[[\"Deck\",\"Num\",\"Side\"]]=X[\"Cabin\"].str.split(\"/\",expand = True)\nX.drop(\"Cabin\",axis=1,inplace=True)\nX.drop(\"Name\",axis=1,inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I noticed that Eur","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.33, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.heatmap(X_train.isna())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imputation done by Aus.\n \nAccording to ChatGPT take, \n1. Generally impute training dataset, then apply imputation method to test dataset\n2. Missing datas are considered Missing At Random (MAR). \nMultiple Imputation, Regression, KNN, EM, Hot Deck were recommended. \nAmong them, Regression is recommended although you must consider other techniques at own discretion.\n\nMy take: \n1. Generally, Age, FoodCourt, SPA, VRDeck shows best positive correlation with one another.\nFor negative correlation, train and test differs. It is seen that RoomService, VRDeck, FoodCourt shows best and consistent negative correlation with one another.\nPerhaps they are multivariate data.\n\n2. I would go impute Age, FoodCourt, SPA, VRDeck, RoomService , ShoppingMall. Removed some columns as to explore for any surprising results.\nCurrently, used Regression Imputation.\n\nNot sure why info() and other things are not updated, despite dropping, adding etc.....","metadata":{}},{"cell_type":"code","source":"X_train.sort_values(by=['Age'], ascending = True)\nX_train[['Age', 'FoodCourt','Spa','VRDeck']].head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.sort_values(by = ['Age'], ascending = True)\nX_train[['RoomService', 'VRDeck', 'FoodCourt']].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking of shape to ensure train and valid features and target corresponds correctly.\n\n# X_train.shape # (5824,15)\n# X_valid.shape # (2869,15)\n# y_train.shape # (5824,)\n# y_valid.shape # (2869,)\n\n# It's correct","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head(len(X_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trying Regression Imputation.\n# Assuming X is your DataFrame\nX_train[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']] = X_train[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']].interpolate(method='linear')\nX_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the Correlation Metrics to a model. Feel free to put in yourthought\ndelete =X_train.drop(columns = ['PassengerId', 'HomePlanet','CryoSleep','Destination','VIP', 'Deck', 'Num', 'Side'])\nX_train.info()\n# Imputed the amenities.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding causation of an event. \nI would think that Random Forest/Bagged Tree, LogisticsRegression are possible to use.","metadata":{}},{"cell_type":"code","source":"random_forest = RandomForestRegressor()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistics Regression. Goal is to look at how to encode accordingly","metadata":{}},{"cell_type":"code","source":"#sns.heatmap(X_test.isna())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now will proceed to impute the validation dataset using Austin's imputation method.","metadata":{}},{"cell_type":"code","source":"\nX_valid[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']] = X_valid[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']].interpolate(method='linear')\nX_valid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now will do the same to X_test","metadata":{}},{"cell_type":"code","source":"\nX_test[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']] = X_test[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']].interpolate(method='linear')\nX_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"i think ill remove the name column as it is unnecessary","metadata":{}},{"cell_type":"code","source":"X_test.drop(\"Name\",axis=1,inplace=True)\nX_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(X_test.isna())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[X_train[\"VIP\"]==False][\"Age\"].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[X_train[\"VIP\"]==True][\"Age\"].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(X_train[X_train[\"VIP\"]==False][\"Age\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[\"Age\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"VIP\",hue=\"CryoSleep\",data=X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = len(X_train[(X_train['VIP'] == True) & (X_train['CryoSleep'] == False)])\ncount","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[\"VIP\"].isna()=X_train[X_train[\"CryoSleep\"]==True][\"VIP\"].fillna(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}