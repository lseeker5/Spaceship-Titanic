{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport datetime\nimport math\nimport statistics\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nfrom itertools import product\nfrom math import sqrt\nimport json\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.base import TransformerMixin #gives fit_transform method for free\n\n#Models\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, model_from_json\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout,Reshape\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import clone_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import matthews_corrcoef, balanced_accuracy_score\nfrom sklearn.feature_selection import RFE\n\n#Graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.signal import periodogram\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_pacf","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.220868Z","iopub.execute_input":"2023-09-13T07:14:39.221300Z","iopub.status.idle":"2023-09-13T07:14:39.235209Z","shell.execute_reply.started":"2023-09-13T07:14:39.221266Z","shell.execute_reply":"2023-09-13T07:14:39.233756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random_seed=42","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.237701Z","iopub.execute_input":"2023-09-13T07:14:39.238166Z","iopub.status.idle":"2023-09-13T07:14:39.248201Z","shell.execute_reply.started":"2023-09-13T07:14:39.238126Z","shell.execute_reply":"2023-09-13T07:14:39.247046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample=pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')\ntest=pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\ntrain=pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.249549Z","iopub.execute_input":"2023-09-13T07:14:39.250779Z","iopub.status.idle":"2023-09-13T07:14:39.321052Z","shell.execute_reply.started":"2023-09-13T07:14:39.250732Z","shell.execute_reply":"2023-09-13T07:14:39.320003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets first split our train dataset into the training portions and validation portions\nX = train.drop(columns=['Transported'])\ny = train['Transported']\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.324454Z","iopub.execute_input":"2023-09-13T07:14:39.325078Z","iopub.status.idle":"2023-09-13T07:14:39.338353Z","shell.execute_reply.started":"2023-09-13T07:14:39.325033Z","shell.execute_reply":"2023-09-13T07:14:39.337096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shows number of rows in X_train\nX_train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.339746Z","iopub.execute_input":"2023-09-13T07:14:39.340103Z","iopub.status.idle":"2023-09-13T07:14:39.346140Z","shell.execute_reply.started":"2023-09-13T07:14:39.340074Z","shell.execute_reply":"2023-09-13T07:14:39.345396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Next,lets have a look at our data\n\ndef data_sum(dataframe):\n    nulls=[]\n    count=[]\n    unique=[]\n    null_percentage=[]\n    dtype=[]\n\n    for col in dataframe.columns:\n        nulls.append(dataframe[col].isnull().sum())\n        count.append(dataframe[col].count())\n        unique.append(dataframe[col].nunique())\n        total_rows=dataframe.shape[0]\n        null_percentage.append(dataframe[col].isnull().sum()*100/total_rows)\n        dtype.append(dataframe[col].dtype)\n    \n    summary=pd.DataFrame({\n        'Column': dataframe.columns,\n        'Nulls': nulls,\n        'Non-Null Count': count,\n        'Unique Values': unique,\n        'Null Percentage': null_percentage,\n        'Data Type': dtype\n    })\n\n    return(summary)\n\nX_train_summary=data_sum(X_train)\nprint(X_train_summary)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.346995Z","iopub.execute_input":"2023-09-13T07:14:39.347294Z","iopub.status.idle":"2023-09-13T07:14:39.396758Z","shell.execute_reply.started":"2023-09-13T07:14:39.347268Z","shell.execute_reply":"2023-09-13T07:14:39.395599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets first drop some colums we wont be using\nX_train=X_train.drop(columns=['PassengerId','Name'])\nX_valid=X_valid.drop(columns=['PassengerId','Name'])\ntest=test.drop(columns=['PassengerId','Name'])\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.400167Z","iopub.execute_input":"2023-09-13T07:14:39.400499Z","iopub.status.idle":"2023-09-13T07:14:39.411158Z","shell.execute_reply.started":"2023-09-13T07:14:39.400470Z","shell.execute_reply":"2023-09-13T07:14:39.409875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train['TotalSpending'] = X_train[['FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']].sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.412999Z","iopub.execute_input":"2023-09-13T07:14:39.413346Z","iopub.status.idle":"2023-09-13T07:14:39.420019Z","shell.execute_reply.started":"2023-09-13T07:14:39.413317Z","shell.execute_reply":"2023-09-13T07:14:39.418837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.421011Z","iopub.execute_input":"2023-09-13T07:14:39.421362Z","iopub.status.idle":"2023-09-13T07:14:39.454481Z","shell.execute_reply.started":"2023-09-13T07:14:39.421333Z","shell.execute_reply":"2023-09-13T07:14:39.453306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.corr()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.459340Z","iopub.execute_input":"2023-09-13T07:14:39.459667Z","iopub.status.idle":"2023-09-13T07:14:39.475718Z","shell.execute_reply.started":"2023-09-13T07:14:39.459631Z","shell.execute_reply":"2023-09-13T07:14:39.474614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First lets split into deck num side ","metadata":{}},{"cell_type":"code","source":"X_train[[\"Deck\",\"Num\",\"Side\"]]=X_train[\"Cabin\"].str.split(\"/\",expand = True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.477193Z","iopub.execute_input":"2023-09-13T07:14:39.477667Z","iopub.status.idle":"2023-09-13T07:14:39.501118Z","shell.execute_reply.started":"2023-09-13T07:14:39.477613Z","shell.execute_reply":"2023-09-13T07:14:39.499777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid[[\"Deck\",\"Num\",\"Side\"]]=X_valid[\"Cabin\"].str.split(\"/\",expand = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.502718Z","iopub.execute_input":"2023-09-13T07:14:39.503169Z","iopub.status.idle":"2023-09-13T07:14:39.516433Z","shell.execute_reply.started":"2023-09-13T07:14:39.503135Z","shell.execute_reply":"2023-09-13T07:14:39.515345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"Deck\",\"Num\",\"Side\"]]=test[\"Cabin\"].str.split(\"/\",expand = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.517765Z","iopub.execute_input":"2023-09-13T07:14:39.518408Z","iopub.status.idle":"2023-09-13T07:14:39.539346Z","shell.execute_reply.started":"2023-09-13T07:14:39.518375Z","shell.execute_reply":"2023-09-13T07:14:39.538118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.drop(columns = ['Cabin'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.540882Z","iopub.execute_input":"2023-09-13T07:14:39.541630Z","iopub.status.idle":"2023-09-13T07:14:39.552190Z","shell.execute_reply.started":"2023-09-13T07:14:39.541584Z","shell.execute_reply":"2023-09-13T07:14:39.550573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid.drop(columns = ['Cabin'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.553922Z","iopub.execute_input":"2023-09-13T07:14:39.554993Z","iopub.status.idle":"2023-09-13T07:14:39.564679Z","shell.execute_reply.started":"2023-09-13T07:14:39.554923Z","shell.execute_reply":"2023-09-13T07:14:39.563826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(columns = ['Cabin'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.566002Z","iopub.execute_input":"2023-09-13T07:14:39.566510Z","iopub.status.idle":"2023-09-13T07:14:39.578558Z","shell.execute_reply.started":"2023-09-13T07:14:39.566480Z","shell.execute_reply":"2023-09-13T07:14:39.576900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now im trying to impute using interpolation\n# Age & Amenities has a relationship. The older, the more spending.\nX_train[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']] = X_train[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']].interpolate(method='linear')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.580119Z","iopub.execute_input":"2023-09-13T07:14:39.581758Z","iopub.status.idle":"2023-09-13T07:14:39.602498Z","shell.execute_reply.started":"2023-09-13T07:14:39.581721Z","shell.execute_reply":"2023-09-13T07:14:39.601286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now im trying to impute using interpolation\n# Age & Amenities has a relationship. The older, the more spending.\ntest[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']] = test[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']].interpolate(method='linear')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.603977Z","iopub.execute_input":"2023-09-13T07:14:39.604771Z","iopub.status.idle":"2023-09-13T07:14:39.623167Z","shell.execute_reply.started":"2023-09-13T07:14:39.604722Z","shell.execute_reply":"2023-09-13T07:14:39.621770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now im trying to impute using interpolation\n# Age & Amenities has a relationship. The older, the more spending.\nX_valid[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']] = X_valid[['Age', 'FoodCourt', 'ShoppingMall','Spa', 'VRDeck', 'RoomService']].interpolate(method='linear')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.624649Z","iopub.execute_input":"2023-09-13T07:14:39.625092Z","iopub.status.idle":"2023-09-13T07:14:39.639041Z","shell.execute_reply.started":"2023-09-13T07:14:39.625052Z","shell.execute_reply":"2023-09-13T07:14:39.637858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see how the dataframe is like now\nX_train_2_summary=data_sum(X_train)\nX_train_2_summary","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.640995Z","iopub.execute_input":"2023-09-13T07:14:39.641594Z","iopub.status.idle":"2023-09-13T07:14:39.686323Z","shell.execute_reply.started":"2023-09-13T07:14:39.641553Z","shell.execute_reply":"2023-09-13T07:14:39.685037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we have completed the imputing for numerical features now lets work on the categorical features.\n\n\n\n<!--  --> from sklearn.impute import SimpleImputer. Constant strategy is better for one-hot encoding\n","metadata":{}},{"cell_type":"code","source":"categorical = X_train.dtypes == 'object'\ncategorical","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.688917Z","iopub.execute_input":"2023-09-13T07:14:39.689365Z","iopub.status.idle":"2023-09-13T07:14:39.698560Z","shell.execute_reply.started":"2023-09-13T07:14:39.689326Z","shell.execute_reply":"2023-09-13T07:14:39.697703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look at the columns with categorical data.\nStarting from HomePlanet.","metadata":{}},{"cell_type":"markdown","source":"Aus help to put some imputation for Arden\nArden - Above\nAustin - Below","metadata":{}},{"cell_type":"code","source":"#Created a new feature\nX_train[\"TotalSpending\"]=X_train[[\"RoomService\",\"Spa\",\"FoodCourt\",\"VRDeck\",'ShoppingMall']].sum(axis=1)\nX_valid[\"TotalSpending\"]=X_valid[[\"RoomService\",\"Spa\",\"FoodCourt\",\"VRDeck\",'ShoppingMall']].sum(axis=1)\ntest[\"TotalSpending\"]=test[[\"RoomService\",\"Spa\",\"FoodCourt\",\"VRDeck\",'ShoppingMall']].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.699851Z","iopub.execute_input":"2023-09-13T07:14:39.700828Z","iopub.status.idle":"2023-09-13T07:14:39.716089Z","shell.execute_reply.started":"2023-09-13T07:14:39.700796Z","shell.execute_reply":"2023-09-13T07:14:39.714809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets impute HomePlanet","metadata":{}},{"cell_type":"code","source":"#For rows where [\"Destination\"]==\"PSO J318.5-22\",let the rows with missing values in [\"HomePlanet\"] be imputed with \"Earth\"\nX_train.loc[X_train[\"Destination\"]==\"PSO J318.5-22\",\"HomePlanet\"] = X_train.loc[X_train[\"Destination\"]==\"PSO J318.5-22\",\"HomePlanet\"].fillna(\"Earth\")\n#For rows where [\"Destination\"]==\"55 Cancri e\",let the rows with missing values in [\"HomePlanet\"] be imputed with \"Europa\"\nX_train.loc[X_train[\"Destination\"]==\"55 Cancri e\",\"HomePlanet\"]=X_train.loc[X_train[\"Destination\"]==\"55 Cancri e\",\"HomePlanet\"].fillna(\"Europa\")\n#For rows where [\"Destination\"]==\"TRAPPIST-1e\",let the rows with missing values in [\"HomePlanet\"] be imputed with \"Earth\"\nX_train.loc[X_train[\"Destination\"]==\"TRAPPIST-1e\",\"HomePlanet\"]=X_train.loc[X_train[\"Destination\"]==\"TRAPPIST-1e\",\"HomePlanet\"].fillna(\"Earth\")\nX_train.loc[X_train['Deck']=='A','HomePlanet']=X_train.loc[X_train['Deck']=='A','HomePlanet'].fillna(\"Europa\")\nX_train.loc[X_train['Deck']=='B','HomePlanet']=X_train.loc[X_train['Deck']=='B','HomePlanet'].fillna(\"Europa\")\nX_train.loc[X_train['Deck']=='C','HomePlanet']=X_train.loc[X_train['Deck']=='C','HomePlanet'].fillna(\"Europa\")\nX_train.loc[X_train['Deck']=='T','HomePlanet']=X_train.loc[X_train['Deck']=='T','HomePlanet'].fillna(\"Europa\")\nX_train.loc[X_train['Deck']=='G','HomePlanet']=X_train.loc[X_train['Deck']=='G','HomePlanet'].fillna(\"Earth\")\nX_train.loc[X_train['Destination'] == 'TRAPPIST-1e', 'HomePlanet'] = X_train.loc[X_train['Destination'] == 'TRAPPIST-1e', 'HomePlanet'].fillna('Earth')\nX_train['HomePlanet']=X_train['HomePlanet'].fillna('Earth')\n\nX_valid.loc[X_valid[\"Destination\"]==\"PSO J318.5-22\",\"HomePlanet\"]=X_valid.loc[X_valid[\"Destination\"]==\"PSO J318.5-22\",\"HomePlanet\"].fillna(\"Earth\")\ntest.loc[test[\"Destination\"]==\"PSO J318.5-22\",\"HomePlanet\"]=test.loc[test[\"Destination\"]==\"PSO J318.5-22\",\"HomePlanet\"].fillna(\"Earth\")\nX_valid.loc[X_valid[\"Destination\"]==\"55 Cancri e\",\"HomePlanet\"]=X_valid.loc[X_valid[\"Destination\"]==\"55 Cancri e\",\"HomePlanet\"].fillna(\"Europa\")\ntest.loc[test[\"Destination\"]==\"55 Cancri e\",\"HomePlanet\"]=test.loc[test[\"Destination\"]==\"55 Cancri e\",\"HomePlanet\"].fillna(\"Europa\")\nX_valid.loc[X_valid[\"Destination\"]==\"TRAPPIST-1e\",\"HomePlanet\"]=X_valid.loc[X_valid[\"Destination\"]==\"TRAPPIST-1e\",\"HomePlanet\"].fillna(\"Earth\")\ntest.loc[test[\"Destination\"]==\"TRAPPIST-1e\",\"HomePlanet\"]=test.loc[test[\"Destination\"]==\"TRAPPIST-1e\",\"HomePlanet\"].fillna(\"Earth\")\nX_valid.loc[X_valid['Deck']=='A','HomePlanet']=X_valid.loc[X_valid['Deck']=='A','HomePlanet'].fillna(\"Europa\")\nX_valid.loc[X_valid['Deck']=='B','HomePlanet']=X_valid.loc[X_valid['Deck']=='B','HomePlanet'].fillna(\"Europa\")\nX_valid.loc[X_valid['Deck']=='C','HomePlanet']=X_valid.loc[X_valid['Deck']=='C','HomePlanet'].fillna(\"Europa\")\nX_valid.loc[X_valid['Deck']=='T','HomePlanet']=X_valid.loc[X_valid['Deck']=='T','HomePlanet'].fillna(\"Europa\")\nX_valid.loc[X_valid['Deck']=='G','HomePlanet']=X_valid.loc[X_valid['Deck']=='G','HomePlanet'].fillna(\"Earth\")\nX_valid.loc[X_valid['Destination'] == 'TRAPPIST-1e', 'HomePlanet'] = X_valid.loc[X_valid['Destination'] == 'TRAPPIST-1e', 'HomePlanet'].fillna('Earth')\nX_valid['HomePlanet']=X_valid['HomePlanet'].fillna('Earth')\n\ntest.loc[test['Deck']=='A','HomePlanet']=test.loc[test['Deck']=='A','HomePlanet'].fillna(\"Europa\")\ntest.loc[test['Deck']=='B','HomePlanet']=test.loc[test['Deck']=='B','HomePlanet'].fillna(\"Europa\")\ntest.loc[test['Deck']=='C','HomePlanet']=test.loc[test['Deck']=='C','HomePlanet'].fillna(\"Europa\")\ntest.loc[test['Deck']=='T','HomePlanet']=test.loc[test['Deck']=='T','HomePlanet'].fillna(\"Europa\")\ntest.loc[test['Deck']=='G','HomePlanet']=test.loc[test['Deck']=='G','HomePlanet'].fillna(\"Earth\")\ntest.loc[test['Destination'] == 'TRAPPIST-1e', 'HomePlanet'] = test.loc[test['Destination'] == 'TRAPPIST-1e', 'HomePlanet'].fillna('Earth')\ntest['HomePlanet']=test['HomePlanet'].fillna('Earth')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.717986Z","iopub.execute_input":"2023-09-13T07:14:39.718432Z","iopub.status.idle":"2023-09-13T07:14:39.842986Z","shell.execute_reply.started":"2023-09-13T07:14:39.718402Z","shell.execute_reply":"2023-09-13T07:14:39.841805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets impute CryoSleep","metadata":{}},{"cell_type":"code","source":"X_train.loc[X_train[\"TotalSpending\"]>=5000,\"CryoSleep\"]=X_train.loc[X_train[\"TotalSpending\"]>=5000,\"CryoSleep\"].fillna(False)\nX_train.loc[X_train[\"TotalSpending\"]==0,\"CryoSleep\"]=X_train.loc[X_train[\"TotalSpending\"]==0,\"CryoSleep\"].fillna(True)\nX_train[\"CryoSleep\"]=X_train[\"CryoSleep\"].fillna(False)\n\nX_valid.loc[X_valid[\"TotalSpending\"]>=5000,\"CryoSleep\"]=X_valid.loc[X_valid[\"TotalSpending\"]>=5000,\"CryoSleep\"].fillna(False)\nX_valid.loc[X_valid[\"TotalSpending\"]==0,\"CryoSleep\"]=X_valid.loc[X_valid[\"TotalSpending\"]==0,\"CryoSleep\"].fillna(True)\nX_valid[\"CryoSleep\"]=X_valid[\"CryoSleep\"].fillna(False)\n\ntest.loc[test[\"TotalSpending\"]>=5000,\"CryoSleep\"]=test.loc[test[\"TotalSpending\"]>=5000,\"CryoSleep\"].fillna(False)\ntest.loc[test[\"TotalSpending\"]==0,\"CryoSleep\"]=test.loc[test[\"TotalSpending\"]==0,\"CryoSleep\"].fillna(True)\ntest[\"CryoSleep\"]=test[\"CryoSleep\"].fillna(False)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.849768Z","iopub.execute_input":"2023-09-13T07:14:39.850155Z","iopub.status.idle":"2023-09-13T07:14:39.878780Z","shell.execute_reply.started":"2023-09-13T07:14:39.850115Z","shell.execute_reply":"2023-09-13T07:14:39.877587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets impute Destination","metadata":{}},{"cell_type":"code","source":"X_train.loc[X_train['VIP'] == 'FALSE','Destination'] = X_train.loc[X_train['VIP'] == 'FALSE','Destination'].fillna('TRAPPIST-1e')\nX_train.loc[X_train['HomePlanet'] == 'Earth', 'Destination'] = X_train.loc[X_train['HomePlanet'] == 'Earth', 'Destination'].fillna(method = 'ffill')\nX_train.loc[X_train['HomePlanet'] == 'Earth', 'Destination'] = X_train.loc[X_train['HomePlanet'] == 'Earth', 'Destination'].fillna(method = 'bfill')\nX_train.loc[X_train['HomePlanet'] == 'Europa', 'Destination'] = X_train.loc[X_train['HomePlanet'] == 'Europa', 'Destination'].fillna('55 Cancri e')\nX_train.loc[X_train['HomePlanet'] == 'Mars', 'Destination'] = X_train.loc[X_train['HomePlanet'] == 'Mars', 'Destination'].fillna('TRAPPIST-1e')\nX_train.loc[X_train['HomePlanet'] == 'Earth','Destination'] = X_train.loc[X_train['HomePlanet'] == 'Earth','Destination'].fillna('TRAPPIST-1e')\nX_train.loc[X_train['VIP'] == False, 'Destination'] = X_train.loc[X_train['VIP'] == False, 'Destination'].fillna('TRAPPIST-1e')\nX_train.loc[X_train['HomePlanet'] == 'Earth', 'Destination'] = X_train.loc[X_train['HomePlanet'] == 'Earth', 'Destination'].fillna('TRAPPIST-1e')\n\nX_valid.loc[X_valid['VIP'] == 'FALSE','Destination'] = X_valid.loc[X_valid['VIP'] == 'FALSE','Destination'].fillna('TRAPPIST-1e')\nX_valid.loc[X_valid['HomePlanet'] == 'Earth', 'Destination'] = X_valid.loc[X_valid['HomePlanet'] == 'Earth', 'Destination'].fillna(method = 'ffill')\nX_valid.loc[X_valid['HomePlanet'] == 'Earth', 'Destination'] = X_valid.loc[X_valid['HomePlanet'] == 'Earth', 'Destination'].fillna(method = 'bfill')\nX_valid.loc[X_valid['HomePlanet'] == 'Europa', 'Destination'] = X_valid.loc[X_valid['HomePlanet'] == 'Europa', 'Destination'].fillna('55 Cancri e')\nX_valid.loc[X_valid['HomePlanet'] == 'Mars', 'Destination'] = X_valid.loc[X_valid['HomePlanet'] == 'Mars', 'Destination'].fillna('TRAPPIST-1e')\nX_valid.loc[X_valid['HomePlanet'] == 'Earth','Destination'] = X_valid.loc[X_valid['HomePlanet'] == 'Earth','Destination'].fillna('TRAPPIST-1e')\nX_valid.loc[X_valid['VIP'] == False, 'Destination'] = X_valid.loc[X_valid['VIP'] == False, 'Destination'].fillna('TRAPPIST-1e')\nX_valid.loc[X_valid['HomePlanet'] == 'Earth', 'Destination'] = X_valid.loc[X_valid['HomePlanet'] == 'Earth', 'Destination'].fillna('TRAPPIST-1e')\n\ntest.loc[test['VIP'] == 'FALSE','Destination'] = test.loc[test['VIP'] == 'FALSE','Destination'].fillna('TRAPPIST-1e')\ntest.loc[test['HomePlanet'] == 'Earth', 'Destination'] = test.loc[test['HomePlanet'] == 'Earth', 'Destination'].fillna(method = 'ffill')\ntest.loc[test['HomePlanet'] == 'Earth', 'Destination'] = test.loc[test['HomePlanet'] == 'Earth', 'Destination'].fillna(method = 'bfill')\ntest.loc[test['HomePlanet'] == 'Europa', 'Destination'] = test.loc[test['HomePlanet'] == 'Europa', 'Destination'].fillna('55 Cancri e')\ntest.loc[test['HomePlanet'] == 'Mars', 'Destination'] = test.loc[test['HomePlanet'] == 'Mars', 'Destination'].fillna('TRAPPIST-1e')\ntest.loc[test['HomePlanet'] == 'Earth','Destination'] = test.loc[test['HomePlanet'] == 'Earth','Destination'].fillna('TRAPPIST-1e')\ntest.loc[test['VIP'] == False, 'Destination'] = test.loc[test['VIP'] == False, 'Destination'].fillna('TRAPPIST-1e')\ntest.loc[test['HomePlanet'] == 'Earth', 'Destination'] = test.loc[test['HomePlanet'] == 'Earth', 'Destination'].fillna('TRAPPIST-1e')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.880892Z","iopub.execute_input":"2023-09-13T07:14:39.881643Z","iopub.status.idle":"2023-09-13T07:14:39.994917Z","shell.execute_reply.started":"2023-09-13T07:14:39.881598Z","shell.execute_reply":"2023-09-13T07:14:39.993223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets impute VIP","metadata":{}},{"cell_type":"code","source":"X_train.loc[X_train['TotalSpending'] >= 5000, 'VIP'] = X_train.loc[X_train['TotalSpending'] >= 5000, 'VIP'].fillna(True)\nX_train.loc[X_train['TotalSpending'] < 5000, 'VIP'] = X_train.loc[X_train['TotalSpending'] < 5000, 'VIP'].fillna(False)\n\nX_valid.loc[X_valid['TotalSpending'] >= 5000, 'VIP'] = X_valid.loc[X_valid['TotalSpending'] >= 5000, 'VIP'].fillna(True)\nX_valid.loc[X_valid['TotalSpending'] < 5000, 'VIP'] = X_valid.loc[X_valid['TotalSpending'] < 5000, 'VIP'].fillna(False)\n\ntest.loc[test['TotalSpending'] >= 5000, 'VIP'] = test.loc[test['TotalSpending'] >= 5000, 'VIP'].fillna(True)\ntest.loc[test['TotalSpending'] < 5000, 'VIP'] = test.loc[test['TotalSpending'] < 5000, 'VIP'].fillna(False)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:39.996824Z","iopub.execute_input":"2023-09-13T07:14:39.997248Z","iopub.status.idle":"2023-09-13T07:14:40.019794Z","shell.execute_reply.started":"2023-09-13T07:14:39.997216Z","shell.execute_reply":"2023-09-13T07:14:40.018583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets impute Deck","metadata":{}},{"cell_type":"code","source":"#Deck has a uneven distribution with deck G and F having a far larger value then the other decks. Lets just impute all nulls  to be F\nX_train['Deck']=X_train['Deck'].fillna('F')\nX_valid['Deck']=X_valid['Deck'].fillna('F')\ntest['Deck']=test['Deck'].fillna('F')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.021453Z","iopub.execute_input":"2023-09-13T07:14:40.021871Z","iopub.status.idle":"2023-09-13T07:14:40.033422Z","shell.execute_reply.started":"2023-09-13T07:14:40.021835Z","shell.execute_reply":"2023-09-13T07:14:40.032321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets  change datatype then impute Num","metadata":{}},{"cell_type":"code","source":"#Convert ['Num'] to numerical feature\nX_train['Num'] = X_train['Num'].str.replace(' ', '').astype(float)\nprint(f\"There are initially {X_train['Num'].isna().sum()} nulls\")\n#Num has too much unique values and we cant do simple imputation because each unique value has such a low count that setting all 72 nulls to that value will skew the results so we just randomly impute the nulls using the existing values \nnum_values = X_train['Num'].dropna().values\n\nX_valid['Num'] = X_valid['Num'].str.replace(' ', '').astype(float)\nprint(f\"There are initially {X_valid['Num'].isna().sum()} nulls\")\n#Num has too much unique values and we cant do simple imputation because each unique value has such a low count that setting all 72 nulls to that value will skew the results so we just randomly impute the nulls using the existing values \nnum_values = X_valid['Num'].dropna().values\n\ntest['Num'] = test['Num'].str.replace(' ', '').astype(float)\nprint(f\"There are initially {test['Num'].isna().sum()} nulls\")\n#Num has too much unique values and we cant do simple imputation because each unique value has such a low count that setting all 72 nulls to that value will skew the results so we just randomly impute the nulls using the existing values \nnum_values = test['Num'].dropna().values\n\n\n# Shuffle non-missing values multiple times\nnum_shuffled = num_values.copy()  # Make a copy to shuffle multiple times\nnp.random.shuffle(num_shuffled)\nnum_shuffled=pd.Series(num_shuffled)\n\nnon_numeric_values = num_shuffled[np.isnan(num_shuffled)]\nif non_numeric_values.size == 0:\n    print(\"All values in num_shuffled are numerical.\")\nelse:\n    print(f\"There are {non_numeric_values.size} non-numeric values in num_shuffled.\")\nnum_unique_values = np.unique(num_shuffled).size\nprint(f\"There are {num_unique_values} unique values in the num_shuffled array.\")\n\n\n# Fill missing values with the shuffled values\nX_train['Num']=X_train['Num'].fillna(num_shuffled)\nprint(f\"Now there are {X_train['Num'].isna().sum()} nulls\")\n\nX_valid['Num']=X_valid['Num'].fillna(num_shuffled)\n\ntest['Num']=test['Num'].fillna(num_shuffled)\n                                  \n#for some reason the number of nulls dropped to 34 but wont drop further, im gonna impute the last 34 nulls with the most frequent value for now until I can solve why\nX_train['Num']=X_train['Num'].fillna(82)\nprint(f\"Now there are {X_train['Num'].isna().sum()} nulls\")\n\nX_valid['Num']=X_valid['Num'].fillna(82)\nprint(f\"Now there are {X_valid['Num'].isna().sum()} nulls\")\n\ntest['Num']=test['Num'].fillna(82)\nprint(f\"Now there are {test['Num'].isna().sum()} nulls\")\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.036209Z","iopub.execute_input":"2023-09-13T07:14:40.037051Z","iopub.status.idle":"2023-09-13T07:14:40.079116Z","shell.execute_reply.started":"2023-09-13T07:14:40.037009Z","shell.execute_reply":"2023-09-13T07:14:40.077652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets impute side","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For Side, they are almost equal and rightfully so, lets just impute the nulls equally\nfor value in ['S','P']:\n    null_indices = X_train[X_train['Side'].isnull()].sample(71).index\n    X_train.loc[null_indices, 'Side'] = value\n    \nfor value in ['S','P']:\n    null_indices = X_valid[X_valid['Side'].isnull()].sample(28).index\n    X_valid.loc[null_indices, 'Side'] = value\nX_valid.loc[X_valid[X_valid[\"Side\"].isna()].index,\"Side\"]=\"S\"\n    \nfor value in ['S','P']:\n    null_indices = test[test['Side'].isnull()].sample(50).index\n    test.loc[null_indices, 'Side'] = value    ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.081075Z","iopub.execute_input":"2023-09-13T07:14:40.081525Z","iopub.status.idle":"2023-09-13T07:14:40.110200Z","shell.execute_reply.started":"2023-09-13T07:14:40.081481Z","shell.execute_reply":"2023-09-13T07:14:40.109027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid.loc[X_valid[X_valid[\"Side\"].isna()].index,\"Side\"]=\"S\"","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.113106Z","iopub.execute_input":"2023-09-13T07:14:40.113854Z","iopub.status.idle":"2023-09-13T07:14:40.121441Z","shell.execute_reply.started":"2023-09-13T07:14:40.113809Z","shell.execute_reply":"2023-09-13T07:14:40.120490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_3_summary=data_sum(X_valid)\nX_train_3_summary","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.122748Z","iopub.execute_input":"2023-09-13T07:14:40.123569Z","iopub.status.idle":"2023-09-13T07:14:40.160758Z","shell.execute_reply.started":"2023-09-13T07:14:40.123527Z","shell.execute_reply":"2023-09-13T07:14:40.159942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imputation complete\n<br>\nNow lets encode\n<br>\nCardinality:\n<br>\nLow: 2-5\n<br>\nModerate: 6-46\n<br>\nHigh: 50 and above","metadata":{}},{"cell_type":"markdown","source":"HomePlanet:Label Encoding\n<br>\nCryoSleep:Label Encoding\n<br>\nDestination:Label Encoding\n<br>\nVIP:Label Encoding\n<br>\nDeck:OHE\n<br>\nSide:Label Encoding\n<br>\nRemaining numerical features:Standardization","metadata":{}},{"cell_type":"code","source":"\nclass MyLabelEncoder(TransformerMixin):\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelEncoder(*args, **kwargs)\n    def fit(self, x, y=0):\n        print(f\" The shape of the fitted is {x.shape}\")\n        print(f\" The shape of the y is {y}\")\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        print(x.shape)\n        return self.encoder.transform(x)\n    def fit_transform(self, x, y=0):\n        #print(f\" The shape of the fitted is {x.shape}\")\n        #x is a dataframe\n        columns=x.columns\n        transformed_features=[]\n        for column in columns:\n            #print(f\" The column is {column}\")\n            feature_column = x[column]\n            #print(f\" Each column has a shape of {feature_column.shape}\")\n            self.encoder.fit(feature_column)\n            new_feature_column=self.encoder.transform(feature_column)\n            transformed_features.append(new_feature_column)\n        transformed_features=np.transpose(transformed_features)\n        return transformed_features\n        \n        \n            \n            \n            \n            \n        \n            \n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.162186Z","iopub.execute_input":"2023-09-13T07:14:40.162899Z","iopub.status.idle":"2023-09-13T07:14:40.175399Z","shell.execute_reply.started":"2023-09-13T07:14:40.162856Z","shell.execute_reply":"2023-09-13T07:14:40.174311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(X_train['Deck'].value_counts())\n#print(X_valid['Deck'].value_counts())\n#print(test['Deck'].value_counts())\n\nl_c_c_f=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Side\"]\nm_c_c_f=['Deck']\nn_f=X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n\n\n# Create transformers for categorical features\nl_c_categorical_transformer = Pipeline(steps=[\n    ('encoder', MyLabelEncoder())  \n])\n\nm_c_categorical_transformer = Pipeline(steps=[\n    ('encoder',OneHotEncoder(handle_unknown='ignore'))  \n])\n\n\n# Create transformers for numerical features\nnumerical_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\n\n# Combine transformers using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, n_f),\n        ('m_c_cat', m_c_categorical_transformer, m_c_c_f),\n        ('l_c_cat', l_c_categorical_transformer, l_c_c_f)\n    ])\n\n# Create the preprocessing pipeline\npreprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\n# Apply the preprocessing pipeline to your data\nencoded_data_X_train = preprocessing_pipeline.fit_transform(X_train)\nprint(encoded_data_X_train.shape)\ncolumns=np.concatenate((n_f,['A','B','C','D','E','F','G','T'] , l_c_c_f))\n# Convert the transformed data back to a DataFrame\nX_train_encoded = pd.DataFrame(encoded_data_X_train,columns=columns)\n\n#Lets fit the pipeline to X_valid and test too\nencoded_data_X_valid = preprocessing_pipeline.fit_transform(X_valid)\nprint(encoded_data_X_valid.shape)\ncolumns=np.concatenate((n_f,['A','B','C','D','E','F','G',] , l_c_c_f))\nX_valid = pd.DataFrame(encoded_data_X_valid,columns=columns)\nX_valid['T']=0\n# Get the column you want to move\ncolumn_to_move = X_valid.pop('T')\n# Insert the column in the 16th position\nX_valid.insert(15, 'T', column_to_move)\nencoded_data_test = preprocessing_pipeline.fit_transform(test)\ncolumns=np.concatenate((n_f,['A','B','C','D','E','F','G','T'] , l_c_c_f))\ntest = pd.DataFrame(encoded_data_test,columns=columns)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.177374Z","iopub.execute_input":"2023-09-13T07:14:40.178049Z","iopub.status.idle":"2023-09-13T07:14:40.255827Z","shell.execute_reply.started":"2023-09-13T07:14:40.177831Z","shell.execute_reply":"2023-09-13T07:14:40.254669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_encoded","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.258086Z","iopub.execute_input":"2023-09-13T07:14:40.258738Z","iopub.status.idle":"2023-09-13T07:14:40.302106Z","shell.execute_reply.started":"2023-09-13T07:14:40.258699Z","shell.execute_reply":"2023-09-13T07:14:40.301007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fantastic we are now done with encoding so lets move on to polynomial features, thing is we dont know how many degrees is the most optimal so lets just create multiple sets of different degrees and see which degree performs the best","metadata":{}},{"cell_type":"markdown","source":"optimal_num_of_features=[]\nbest_scores_for_each_degree=[]\n\nmax_degree=3#Theres already 2024 features when degree is 3\n\nfor degree in range(1,max_degree+1):\n    poly = PolynomialFeatures(degree=degree)\n    X_train_poly=poly.fit_transform(X_train_encoded)\n    X_valid_poly=poly.transform(X_valid)\n    max_num_of_features=X_train_poly.shape[1]\n    num_of_features=X_train_poly.shape[1]-1\n    rf_classifier = RandomForestClassifier(random_state=42)\n    val_scores=[]\n    new_X_train=X_train_poly\n    new_X_valid=X_valid_poly\n    full_features=list(sorted(range(1, num_of_features), reverse=True))\n    \n    while num_of_features>1:\n        rfe = RFE(estimator=rf_classifier, n_features_to_select=num_of_features)\n        rfe.fit(new_X_train, y_train)\n        selected_features = rfe.support_\n        #print(selected_features)\n        #print(X_train_poly.shape)\n        new_X_train=new_X_train[:,selected_features]\n        #print(new_X_train.shape)\n        #print(X_valid.shape)\n        new_X_valid=new_X_valid[:,selected_features]\n        rf_classifier.fit(new_X_train,y_train)\n        y_pred=rf_classifier.predict(new_X_valid)\n        mcc = matthews_corrcoef(y_valid, y_pred)\n        val_scores.append(mcc)\n        num_of_features-=1\n        print(f\"New number of features are {num_of_features}\")\n        \n    sns.scatterplot(x=full_features, y=val_scores)\n    # Add labels and a title\n    plt.xlabel('Number of features')\n    plt.ylabel('Validation Scores')\n    plt.title('Variation of MCC with number of features')\n    # Show the plot\n    plt.show()\n    \n    max_index = val_scores.index(max(val_scores))\n    best_number_of_features=X_train_poly.shape[1]-1-max_index\n    optimal_num_of_features.append(best_number_of_features)\n    best_scores_for_each_degree.append(max(val_scores))\n    print(f\"Degree {degree} done\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:17:10.718686Z","iopub.execute_input":"2023-09-09T05:17:10.719012Z","iopub.status.idle":"2023-09-09T14:46:49.669198Z","shell.execute_reply.started":"2023-09-09T05:17:10.718988Z","shell.execute_reply":"2023-09-09T14:46:49.667932Z"}}},{"cell_type":"markdown","source":"Lets start choosing our models, I think random forest is one good option to try out","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.303440Z","iopub.execute_input":"2023-09-13T07:14:40.303785Z","iopub.status.idle":"2023-09-13T07:14:40.311938Z","shell.execute_reply.started":"2023-09-13T07:14:40.303755Z","shell.execute_reply":"2023-09-13T07:14:40.310462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], # Regularization parameter \n              'penalty': ['l1', 'l2'], # Regularization type \n              'solver': ['liblinear', 'saga']} # Algorithm to use for optimization","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.313458Z","iopub.execute_input":"2023-09-13T07:14:40.313878Z","iopub.status.idle":"2023-09-13T07:14:40.326537Z","shell.execute_reply.started":"2023-09-13T07:14:40.313836Z","shell.execute_reply":"2023-09-13T07:14:40.325461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg = LogisticRegression()\ngrid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1) \ngrid_search.fit(X_train_encoded, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:40.329857Z","iopub.execute_input":"2023-09-13T07:14:40.330216Z","iopub.status.idle":"2023-09-13T07:14:46.715791Z","shell.execute_reply.started":"2023-09-13T07:14:40.330186Z","shell.execute_reply":"2023-09-13T07:14:46.714345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = grid_search.best_params_ \nbest_model = grid_search.best_estimator_\nbest_model","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:14:46.717340Z","iopub.execute_input":"2023-09-13T07:14:46.717786Z","iopub.status.idle":"2023-09-13T07:14:46.727463Z","shell.execute_reply.started":"2023-09-13T07:14:46.717747Z","shell.execute_reply":"2023-09-13T07:14:46.726303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ndegrees = [1, 2, 3]  # List of polynomial degrees to try\n\nbest_degree = None\nbest_accuracy = 0\n\nfor degree in degrees:\n    # Create polynomial features\n    poly = PolynomialFeatures(degree=degree)\n    X_train_poly = poly.fit_transform(X_train_encoded)\n    X_valid_poly = poly.transform(X_valid)\n\n    # Fit a logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train_poly, y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_valid_poly)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_valid, y_pred)\n\n    # Check if this degree gives the best accuracy\n\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_degree = degree\n\n    print(f\"Degree {degree} Accuracy: {accuracy}\")\n\nprint(f\"Best Degree: {best_degree} (Accuracy: {best_accuracy})\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:29:59.686286Z","iopub.execute_input":"2023-09-13T07:29:59.686720Z","iopub.status.idle":"2023-09-13T07:30:01.593248Z","shell.execute_reply.started":"2023-09-13T07:29:59.686689Z","shell.execute_reply":"2023-09-13T07:30:01.592084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so according to the polynomial features, degree 2 yields the highest accuracy.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}